{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b8f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import Encoder, MuSigmaEncoder, Decoder\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from utils import img_mask_to_np_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916b867",
   "metadata": {},
   "source": [
    "The following class implements Neural Process for functions of arbitrary dimensions.\n",
    "\n",
    "Parameters\n",
    " ----------\n",
    "x_dim : int\n",
    "    Dimension of x values.\n",
    "\n",
    "y_dim : int\n",
    "    Dimension of y values.\n",
    "\n",
    "r_dim : int\n",
    "    Dimension of output representation r.\n",
    "\n",
    "z_dim : int\n",
    "    Dimension of latent variable z.\n",
    "\n",
    "h_dim : int\n",
    "    Dimension of hidden layer in encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4777fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralProcess(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, r_dim, z_dim, h_dim):\n",
    "        super(NeuralProcess, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.r_dim = r_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        # Initialize networks\n",
    "        self.xy_to_r = Encoder(x_dim, y_dim, h_dim, r_dim)\n",
    "        self.r_to_mu_sigma = MuSigmaEncoder(r_dim, z_dim)\n",
    "        self.xz_to_y = Decoder(x_dim, z_dim, h_dim, y_dim)\n",
    "\n",
    "    def aggregate(self, r_i):\n",
    "        \"\"\"\n",
    "        Aggregates representations for every (x_i, y_i) pair into a single\n",
    "        representation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        r_i : torch.Tensor\n",
    "            Shape (batch_size, num_points, r_dim)\n",
    "        \"\"\"\n",
    "        return torch.mean(r_i, dim=1)\n",
    "\n",
    "    def xy_to_mu_sigma(self, x, y):\n",
    "        \"\"\"\n",
    "        Maps (x, y) pairs into the mu and sigma parameters defining the normal\n",
    "        distribution of the latent variables z.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape (batch_size, num_points, x_dim)\n",
    "\n",
    "        y : torch.Tensor\n",
    "            Shape (batch_size, num_points, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, num_points, _ = x.size()\n",
    "        # Flatten tensors, as encoder expects one dimensional inputs\n",
    "        x_flat = x.view(batch_size * num_points, self.x_dim)\n",
    "        y_flat = y.contiguous().view(batch_size * num_points, self.y_dim)\n",
    "        # Encode each point into a representation r_i\n",
    "        r_i_flat = self.xy_to_r(x_flat, y_flat)\n",
    "        # Reshape tensors into batches\n",
    "        r_i = r_i_flat.view(batch_size, num_points, self.r_dim)\n",
    "        # Aggregate representations r_i into a single representation r\n",
    "        r = self.aggregate(r_i)\n",
    "        # Return parameters of distribution\n",
    "        return self.r_to_mu_sigma(r)\n",
    "\n",
    "    def forward(self, x_context, y_context, x_target, y_target=None):\n",
    "        \"\"\"\n",
    "        Given context pairs (x_context, y_context) and target points x_target,\n",
    "        returns a distribution over target points y_target.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_context : torch.Tensor\n",
    "            Shape (batch_size, num_context, x_dim). Note that x_context is a\n",
    "            subset of x_target.\n",
    "\n",
    "        y_context : torch.Tensor\n",
    "            Shape (batch_size, num_context, y_dim)\n",
    "\n",
    "        x_target : torch.Tensor\n",
    "            Shape (batch_size, num_target, x_dim)\n",
    "\n",
    "        y_target : torch.Tensor or None\n",
    "            Shape (batch_size, num_target, y_dim). Only used during training.\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        We follow the convention given in \"Empirical Evaluation of Neural\n",
    "        Process Objectives\" where context is a subset of target points. This was\n",
    "        shown to work best empirically.\n",
    "        \"\"\"\n",
    "        # Infer quantities from tensor dimensions\n",
    "        batch_size, num_context, x_dim = x_context.size()\n",
    "        _, num_target, _ = x_target.size()\n",
    "        _, _, y_dim = y_context.size()\n",
    "\n",
    "        if self.training:\n",
    "            # Encode target and context (context needs to be encoded to\n",
    "            # calculate kl term)\n",
    "            mu_target, sigma_target = self.xy_to_mu_sigma(x_target, y_target)\n",
    "            mu_context, sigma_context = self.xy_to_mu_sigma(x_context, y_context)\n",
    "            # Sample from encoded distribution using reparameterization trick\n",
    "            q_target = Normal(mu_target, sigma_target)\n",
    "            q_context = Normal(mu_context, sigma_context)\n",
    "            z_sample = q_target.rsample()\n",
    "            # Get parameters of output distribution\n",
    "            y_pred_mu, y_pred_sigma = self.xz_to_y(x_target, z_sample)\n",
    "            p_y_pred = Normal(y_pred_mu, y_pred_sigma)\n",
    "\n",
    "            return p_y_pred, q_target, q_context\n",
    "        else:\n",
    "            # At testing time, encode only context\n",
    "            mu_context, sigma_context = self.xy_to_mu_sigma(x_context, y_context)\n",
    "            # Sample from distribution based on context\n",
    "            q_context = Normal(mu_context, sigma_context)\n",
    "            z_sample = q_context.rsample()\n",
    "            # Predict target points based on context\n",
    "            y_pred_mu, y_pred_sigma = self.xz_to_y(x_target, z_sample)\n",
    "            p_y_pred = Normal(y_pred_mu, y_pred_sigma)\n",
    "\n",
    "            return p_y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04bdfc",
   "metadata": {},
   "source": [
    "Wraps regular Neural Process for image processing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : tuple of ints\n",
    "        E.g. (1, 28, 28) or (3, 32, 32)\n",
    "\n",
    "    r_dim : int\n",
    "        Dimension of output representation r.\n",
    "\n",
    "    z_dim : int\n",
    "        Dimension of latent variable z.\n",
    "\n",
    "    h_dim : int\n",
    "        Dimension of hidden layer in encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca31d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralProcessImg(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, r_dim, z_dim, h_dim):\n",
    "        super(NeuralProcessImg, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels, self.height, self.width = img_size\n",
    "        self.r_dim = r_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        self.neural_process = NeuralProcess(x_dim=2, y_dim=self.num_channels,\n",
    "                                            r_dim=r_dim, z_dim=z_dim,\n",
    "                                            h_dim=h_dim)\n",
    "\n",
    "    def forward(self, img, context_mask, target_mask):\n",
    "        \"\"\"\n",
    "        Given an image and masks of context and target points, returns a\n",
    "        distribution over pixel intensities at the target points.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor\n",
    "            Shape (batch_size, channels, height, width)\n",
    "\n",
    "        context_mask : torch.ByteTensor\n",
    "            Shape (batch_size, height, width). Binary mask indicating\n",
    "            the pixels to be used as context.\n",
    "\n",
    "        target_mask : torch.ByteTensor\n",
    "            Shape (batch_size, height, width). Binary mask indicating\n",
    "            the pixels to be used as target.\n",
    "        \"\"\"\n",
    "        x_context, y_context = img_mask_to_np_input(img, context_mask)\n",
    "        x_target, y_target = img_mask_to_np_input(img, target_mask)\n",
    "        return self.neural_process(x_context, y_context, x_target, y_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
